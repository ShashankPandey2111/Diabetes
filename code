
Diabetes.ipynb
Diabetes.ipynb_

[ ]

Start coding or generate with AI.
New Section

[ ]
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm
import sklearn
from sklearn.preprocessing import scale, StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.metrics import confusion_matrix, accuracy_score, mean_squared_error, r2_score, roc_auc_score, roc_curve, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import KFold
import warnings
warnings.simplefilter(action='ignore')
sns.set()
plt.style.use("ggplot")
%matplotlib inline

[ ]
#read the datset from
df=pd.read_csv("diabetes.csv")

[ ]
df.head()


[ ]
#it is supervised
#classifctaion
df.info()
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 768 entries, 0 to 767
Data columns (total 9 columns):
 #   Column                    Non-Null Count  Dtype  
---  ------                    --------------  -----  
 0   Pregnancies               768 non-null    int64  
 1   Glucose                   768 non-null    int64  
 2   BloodPressure             768 non-null    int64  
 3   SkinThickness             768 non-null    int64  
 4   Insulin                   768 non-null    int64  
 5   BMI                       768 non-null    float64
 6   DiabetesPedigreeFunction  768 non-null    float64
 7   Age                       768 non-null    int64  
 8   Outcome                   768 non-null    int64  
dtypes: float64(2), int64(7)
memory usage: 54.1 KB

[ ]
df.columns
Index(['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',
       'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome'],
      dtype='object')

[ ]
# independent feature->'Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',
#        'BMI', 'DiabetesPedigreeFunction', 'Age'
# dependent feature-> outcome

[ ]
df.describe()


[ ]
df.shape
#(rows, cols)
(768, 9)

[ ]
#distrbution of outcmoe variable
df['Outcome'].value_counts()*100/len(df)


[ ]
# plot the varibale of the age variable
plt.figure(figsize=(8,7))
plt.xlabel('Age',fontsize=10)
plt.ylabel ('count',fontsize=10)
df['Age'].hist(edgecolor="black")


[ ]
df['Age'].max()

81

[ ]
df['Age'].min()
21

[ ]
print("MAX AGE:"+str(df['Age'].max()))
print("MIN AGE:"+str(df['Age'].min()))
MAX AGE:81
MIN AGE:21

[ ]
# density graph
fig,ax=plt.subplots(4,2,figsize=(20,20))
sns.distplot(df.Pregnancies, bins=20 ,ax=ax[0,0],color="red")
sns.distplot(df.Glucose , bins=20 ,ax=ax[0,1],color="blue")
sns.distplot(df.BloodPressure , bins=20 ,ax=ax[1,0],color="green")
sns.distplot(df.SkinThickness , bins=20 ,ax=ax[1, 1 ],color="yellow")
sns.distplot(df.Insulin , bins=20 ,ax=ax[2,0],color="orange")
sns.distplot(df.BMI , bins=20 ,ax=ax[2,1],color="pink")
sns.distplot(df.DiabetesPedigreeFunction , bins=20 ,ax=ax[3,0],col


[ ]
df.groupby("Outcome").agg({'Pregnancies':'mean'})


[ ]
df.groupby("Outcome").agg({'Pregnancies':'max'})


[ ]
df.groupby("Outcome").agg({'Glucose':'mean'})


[ ]
df.groupby("Outcome").agg({'Glucose':'max'})


[ ]
# 0 is healthy
# 1 is diabetes

[ ]
f,ax =plt.subplots(1,2,figsize=(18,8))
df['Outcome'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)
ax[0].set_title('target')
ax[0].set_ylabel('')
sns.countplot(x='Outcome' , data=df, ax=ax[1])
ax[1].set_title('Outcome')
plt.show()


[ ]
df.corr()


[ ]
f,ax=plt.subplots(figsize=[20,15])
sns.heatmap(df.corr(), annot =True , fmt ='.2f',ax=ax ,cmap="magma")
ax.set_title("correlation matrix", fontsize= 20)
plt.show()


[ ]
# eda part completed


[ ]
#data preprocessing part

[ ]
df.columns

Index(['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',
       'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome'],
      dtype='object')

[ ]
df[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',
       'BMI', 'DiabetesPedigreeFunction', 'Age']] =df[['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',
       'BMI', 'DiabetesPedigreeFunction', 'Age']].replace(0,np.nan)

[ ]
df.isnull().sum()


[ ]
df.head()



[ ]
import missingno as msno
msno.bar (df,color="green")


[ ]
#filling the missing value with median
def median_target(var):
  temp= df[df[var].notnull()]
  temp = temp[[var,'Outcome']].groupby(['Outcome'])[[var]].median().reset_index()
  return temp

[ ]
columns = df.columns
columns=columns.drop('Outcome')
for i in columns:
  median_target(i)
  df.loc[(df['Outcome']==0 &(df[i].isnull()),i)]=median_target(i)[i][0]
  df.loc[(df['Outcome']==1 &(df[i].isnull()),i)]=median_target(i)[i][1]

[ ]
df.head()



[ ]
df.isnull().sum()


[ ]
p=sns.pairplot(df, hue ="Outcome")


[ ]
# Outlier detection
#iqr
for feature in df:
  Q1=df[feature].quantile(0.25)
  Q3=df[feature].quantile(0.75)
  IQR=Q3-Q1
  lower=Q1-1.5*IQR
  upper=Q3+1.5*IQR
  if df[(df[feature]>upper)].any(axis=None):
    print(feature,"yes")
  else:
      print(feature,"no")
Pregnancies yes
Glucose yes
BloodPressure yes
SkinThickness yes
Insulin yes
BMI yes
DiabetesPedigreeFunction yes
Age yes
Outcome no

[ ]
plt.figure(figsize=(8,7))
sns.boxplot(x=df["Insulin"],color="red")


[ ]
#remove all the outliers
Q1 = df.Insulin.quantile(.25)
Q3 = df.Insulin.quantile(.75)
IQR=Q3-Q1
lower=Q1-1.5*IQR
upper=Q3+1.5*IQR
df.loc[df['Insulin']>upper,"Insulin"]=upper


[ ]
plt.figure(figsize=(8,7))
sns.boxplot(x=df["Insulin"],color="red")


[ ]
from sklearn.neighbors import LocalOutlierFactor
lof =LocalOutlierFactor(n_neighbors=10)
lof.fit_predict(df)

array([-1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,
        1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,
        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,
        1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,
        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,
        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,
        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,
        1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,
        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,
        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,
        1,  1,  1,  1,  1,  1,  1, -1,  1, -1,  1,  1,  1,  1,  1,  1,  1,
        1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,
        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,
        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,
        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,
        1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1, -1,  1,  1, -1,  1,  1,
        1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1,
        1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,
        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,
        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,
        1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,
        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,
        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,
        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,
        1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,
        1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,
        1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,
        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,
        1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,  1,
        1,  1,  1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1,  1,  1,
        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,
        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,
        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,
        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,
        1, -1,  1,  1,  1,  1,  1,  1, -1,  1,  1,  1,  1,  1, -1,  1,  1,
        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,
        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,
        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,
        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,
        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,
        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,
        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,
        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,
        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,
        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,
        1,  1,  1])

[ ]
df.head()


[ ]
plt.figure(figsize=(8,7))
sns.boxplot(x=df["Pregnancies"],color="red")


[ ]
df_scores=lof.negative_outlier_factor_
np.sort(df_scores)[0:20]
array([-1.16082304e+11, -9.93248444e+10, -9.20550681e+10, -8.63282955e+10,
       -8.34831029e+10, -7.98213557e+10, -7.78801348e+10, -6.87343634e+10,
       -6.14340944e+10, -5.90010523e+10, -5.85407284e+10, -5.69885875e+10,
       -5.17473503e+10, -4.74100451e+10, -3.68584239e+10, -3.66768798e+10,
       -1.38419432e+10, -3.38259412e+00, -1.98508660e+00, -1.73010441e+00])

[ ]
threshold=np.sort(df_scores)[7]

[ ]
threshold

np.float64(-68734363356.27762)

[ ]
outlier=df_scores>threshold

[ ]
df=df[outlier]


[ ]
df.head()


[ ]
df.shape
(760, 9)

[ ]
plt.figure(figsize=(8,7))
sns.boxplot(x=df["Pregnancies"],color="red")


[ ]
#feature engeineering
newbmi= pd.Series(["underweight ","normal","overweight","obesity1","obesity2","obesity3"],dtype="category")

[ ]
newbmi


[ ]
df["newbmi"] =newbmi
df.loc[df["BMI"]<18.5,"newbmi"]=newbmi[0]
df.loc[(df["BMI"]>18.5)& df["BMI"]<=24.9 ,"newbmi"]=newbmi[1]
df.loc[(df["BMI"]>18.5)& df["BMI"]<=24.9 ,"newbmi"]=newbmi[2]
df.loc[(df["BMI"]>18.5)& df["BMI"]<=24.9 ,"newbmi"]=newbmi[3]
df.loc[(df["BMI"]>18.5)& df["BMI"]<=24.9 ,"newbmi"]=newbmi[4]
df.loc[df["BMI"]>39.9,"newbmi"]=newbmi[5]

[ ]
df.head()

Double-click (or enter) to edit


[ ]
df["newglucose"] = pd.Series([''] * df.shape[0], dtype='object')  # Initialize as empty strings with the correct length
newglucose = pd.Series(["low", "normal", "overweight", "secret", "high"], dtype="category")

df.loc[df["Glucose"] <= 70, "newglucose"] = newglucose[0]  #Glucose <= 70
df.loc[(df["Glucose"] > 70) & (df["Glucose"] <= 99), "newglucose"] = newglucose[1]  #70 < Glucose <= 99
df.loc[(df["Glucose"] > 99) & (df["Glucose"] <= 126), "newglucose"] = newglucose[2]  #99 < Glucose <= 126
df.loc[df["Glucose"] > 126, "newglucose"] = newglucose[3]  #Glucose > 126

[ ]
df.head()


[ ]
df=pd.get_dummies(df, columns=["newbmi","newglucose"],drop_first=True)

[ ]
df.head()


[ ]
df.columns

Index(['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',
       'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome', 'newbmi_obesity1',
       'newbmi_obesity2', 'newbmi_obesity3', 'newbmi_overweight',
       'newbmi_underweight ', 'newglucose_overweight', 'newglucose_secret'],
      dtype='object')

[ ]
categorical_df=df[['newbmi_obesity1',
       'newbmi_obesity2', 'newbmi_obesity3', 'newbmi_overweight',
       'newbmi_underweight ', 'newglucose_overweight', 'newglucose_secret']]

[ ]
categorical_df.head()


[ ]
y=df['Outcome']
x= df.drop(['Outcome','newbmi_obesity1',
       'newbmi_obesity2', 'newbmi_obesity3', 'newbmi_overweight',
       'newbmi_underweight ', 'newglucose_overweight', 'newglucose_secret'],axis=1)

[ ]
cols=x.columns
index=x.index

[ ]
x.head()


[ ]
from sklearn.preprocessing import RobustScaler
transform =RobustScaler().fit(x)
x=transform.transform(x)
x=pd.DataFrame(x,columns=cols,index=index)

[ ]
x.head()


[ ]
x=pd.concat([x,categorical_df],axis=1)


[ ]
x.head()


[ ]
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=0)

[ ]
scale=StandardScaler()
x_train=scale.fit_transform(x_train)
x_test=scale.transform(x_test)

[ ]
# chine learning algorithm


[ ]
log_reg=LogisticRegression()
log_reg.fit(x_train,y_train)


[ ]
y_pred=log_reg.predict(x_test)

[ ]
accuracy_score(y_test,y_pred)
0.9539473684210527

[ ]
accuracy_score(y_test,log_reg.predict(x_test))
0.9539473684210527

[ ]
confusion_matrix(y_test,y_pred)
array([[101,   0],
       [  7,  44]])

[ ]
classifictaion_report=classification_report(y_test,y_pred)
print(classifictaion_report)
              precision    recall  f1-score   support

           0       0.94      1.00      0.97       101
           1       1.00      0.86      0.93        51

    accuracy                           0.95       152
   macro avg       0.97      0.93      0.95       152
weighted avg       0.96      0.95      0.95       152


[ ]
knn=KNeighborsClassifier()
knn.fit(x_train,y_train)
y_pred=knn.predict(x_test)
print(accuracy_score(y_test,knn.predict(x_test)))
print(accuracy_score(y_train,knn.predict(x_train)))
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))



1.0
0.993421052631579
[[101   0]
 [  0  51]]
              precision    recall  f1-score   support

           0       1.00      1.00      1.00       101
           1       1.00      1.00      1.00        51

    accuracy                           1.00       152
   macro avg       1.00      1.00      1.00       152
weighted avg       1.00      1.00      1.00       152


[ ]
svc=SVC(probability=True)
parameter={
    "gamma":[0.0001,0.001,0.01,0.1],
    "C":[0.01,0.05,0.5,0.01,1,10,15,20]
}
grid_search=GridSearchCV(svc,parameter)
grid_search.fit(x_train,y_train)


[ ]
grid_search.best_params_
{'C': 0.5, 'gamma': 0.1}

[ ]
grid_search.best_score_
np.float64(0.9950819672131146)

[ ]
svc=SVC(C=.5,gamma=.1,probability=True)
svc.fit(x_train,y_train)
y_pred=svc.predict(x_test)
print(accuracy_score(y_test,knn.predict(x_test)))
print(accuracy_score(y_train,knn.predict(x_train)))
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))
1.0
0.993421052631579
[[101   0]
 [  0  51]]
              precision    recall  f1-score   support

           0       1.00      1.00      1.00       101
           1       1.00      1.00      1.00        51

    accuracy                           1.00       152
   macro avg       1.00      1.00      1.00       152
weighted avg       1.00      1.00      1.00       152


[ ]
DT=DecisionTreeClassifier()
DT.fit(x_train,y_train)
y_pred=DT.predict(x_test)
print(accuracy_score(y_test,knn.predict(x_test)))
print(accuracy_score(y_train,knn.predict(x_train)))
print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))
1.0
0.993421052631579
[[101   0]
 [  0  51]]
              precision    recall  f1-score   support

           0       1.00      1.00      1.00       101
           1       1.00      1.00      1.00        51

    accuracy                           1.00       152
   macro avg       1.00      1.00      1.00       152
weighted avg       1.00      1.00      1.00       152


[ ]

Start coding or generate with AI.
Colab paid products - Cancel contracts here
